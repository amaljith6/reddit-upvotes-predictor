{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('new_data.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>archived</th>\n",
       "      <th>...</th>\n",
       "      <th>link_id</th>\n",
       "      <th>gilded</th>\n",
       "      <th>edited</th>\n",
       "      <th>ups</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>id</th>\n",
       "      <th>downs</th>\n",
       "      <th>controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_c110jjw</td>\n",
       "      <td>1285891201</td>\n",
       "      <td>hbetx9</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>Can you give a reference for this. I don't bel...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dl2dp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1426494582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c1112h4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t1_c10zgl5</td>\n",
       "      <td>1285891201</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>t5_2qlqh</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Android</td>\n",
       "      <td>1</td>\n",
       "      <td>I have this and it works well.</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dkzvj</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1426494582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c1112h5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_dkyy9</td>\n",
       "      <td>1285891201</td>\n",
       "      <td>UnderwaterJones</td>\n",
       "      <td>t5_2qh63</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>1</td>\n",
       "      <td>I like Ron Paul, but I wish the guy would wear...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dkyy9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1426494582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c1112h6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t1_c110zc9</td>\n",
       "      <td>1285891202</td>\n",
       "      <td>Pokaris</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>0</td>\n",
       "      <td>To produce what we currently produce using the...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dkz7y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1426494582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c1112h7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_c10y7x1</td>\n",
       "      <td>1285891202</td>\n",
       "      <td>iheartthrowaway</td>\n",
       "      <td>t5_2qh3p</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sex</td>\n",
       "      <td>2</td>\n",
       "      <td>This was shockingly coincidental, but I squirt...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_d6rg9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1426494582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c1112h8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>t1_c110ci6</td>\n",
       "      <td>1285900174</td>\n",
       "      <td>FoolsRun</td>\n",
       "      <td>t5_2qh0u</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pics</td>\n",
       "      <td>2</td>\n",
       "      <td>I didn't even know we had a cabin.</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dl10j</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1426494783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c111hz6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>t3_dlb12</td>\n",
       "      <td>1285900174</td>\n",
       "      <td>spatchcock</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1</td>\n",
       "      <td>Show the ladies and acquire all the pussy with...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dlb12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1426494783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c111hz7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>t3_dlawe</td>\n",
       "      <td>1285900174</td>\n",
       "      <td>invalidusername2</td>\n",
       "      <td>t5_2r9vp</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trees</td>\n",
       "      <td>1</td>\n",
       "      <td>im looking to try this pretty soon. how is it?</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dlawe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1426494783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c111hz8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>t1_c111hgt</td>\n",
       "      <td>1285900176</td>\n",
       "      <td>Andrewr05</td>\n",
       "      <td>t5_2qh2p</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>atheism</td>\n",
       "      <td>2</td>\n",
       "      <td>&amp;gt;*\"The Holy Ghost was so fucking cool. It m...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dl2be</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1426494783</td>\n",
       "      <td>no-knight</td>\n",
       "      <td>Atheist</td>\n",
       "      <td>c111hz9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>t1_c1119da</td>\n",
       "      <td>1285900176</td>\n",
       "      <td>tau-lepton</td>\n",
       "      <td>t5_2qh3p</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sex</td>\n",
       "      <td>5</td>\n",
       "      <td>No, seriously.</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_dl2ng</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1426494783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c111hza</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        parent_id  created_utc            author subreddit_id  score_hidden  \\\n",
       "0      t1_c110jjw   1285891201            hbetx9     t5_2qh6p         False   \n",
       "1      t1_c10zgl5   1285891201         [deleted]     t5_2qlqh         False   \n",
       "2        t3_dkyy9   1285891201   UnderwaterJones     t5_2qh63         False   \n",
       "3      t1_c110zc9   1285891202           Pokaris         t5_6         False   \n",
       "4      t1_c10y7x1   1285891202   iheartthrowaway     t5_2qh3p         False   \n",
       "...           ...          ...               ...          ...           ...   \n",
       "19995  t1_c110ci6   1285900174          FoolsRun     t5_2qh0u         False   \n",
       "19996    t3_dlb12   1285900174        spatchcock     t5_2qh1i         False   \n",
       "19997    t3_dlawe   1285900174  invalidusername2     t5_2r9vp         False   \n",
       "19998  t1_c111hgt   1285900176         Andrewr05     t5_2qh2p         False   \n",
       "19999  t1_c1119da   1285900176        tau-lepton     t5_2qh3p         False   \n",
       "\n",
       "      distinguished     subreddit  score  \\\n",
       "0               NaN  Conservative      3   \n",
       "1               NaN       Android      1   \n",
       "2               NaN   Libertarian      1   \n",
       "3               NaN    reddit.com      0   \n",
       "4               NaN           sex      2   \n",
       "...             ...           ...    ...   \n",
       "19995           NaN          pics      2   \n",
       "19996           NaN     AskReddit      1   \n",
       "19997           NaN         trees      1   \n",
       "19998           NaN       atheism      2   \n",
       "19999           NaN           sex      5   \n",
       "\n",
       "                                                    body  archived  ...  \\\n",
       "0      Can you give a reference for this. I don't bel...      True  ...   \n",
       "1                         I have this and it works well.      True  ...   \n",
       "2      I like Ron Paul, but I wish the guy would wear...      True  ...   \n",
       "3      To produce what we currently produce using the...      True  ...   \n",
       "4      This was shockingly coincidental, but I squirt...      True  ...   \n",
       "...                                                  ...       ...  ...   \n",
       "19995                 I didn't even know we had a cabin.      True  ...   \n",
       "19996  Show the ladies and acquire all the pussy with...      True  ...   \n",
       "19997     im looking to try this pretty soon. how is it?      True  ...   \n",
       "19998  &gt;*\"The Holy Ghost was so fucking cool. It m...      True  ...   \n",
       "19999                                     No, seriously.      True  ...   \n",
       "\n",
       "        link_id gilded  edited  ups  retrieved_on  author_flair_css_class  \\\n",
       "0      t3_dl2dp      0       0    3    1426494582                     NaN   \n",
       "1      t3_dkzvj      0       0    1    1426494582                     NaN   \n",
       "2      t3_dkyy9      0       0    1    1426494582                     NaN   \n",
       "3      t3_dkz7y      0       0    0    1426494582                     NaN   \n",
       "4      t3_d6rg9      0       0    2    1426494582                     NaN   \n",
       "...         ...    ...     ...  ...           ...                     ...   \n",
       "19995  t3_dl10j      0       0    2    1426494783                     NaN   \n",
       "19996  t3_dlb12      0       0    1    1426494783                     NaN   \n",
       "19997  t3_dlawe      0       0    1    1426494783                     NaN   \n",
       "19998  t3_dl2be      0       0    2    1426494783               no-knight   \n",
       "19999  t3_dl2ng      0       0    5    1426494783                     NaN   \n",
       "\n",
       "      author_flair_text       id downs  controversiality  \n",
       "0                   NaN  c1112h4     0                 0  \n",
       "1                   NaN  c1112h5     0                 0  \n",
       "2                   NaN  c1112h6     0                 0  \n",
       "3                   NaN  c1112h7     0                 0  \n",
       "4                   NaN  c1112h8     0                 0  \n",
       "...                 ...      ...   ...               ...  \n",
       "19995               NaN  c111hz6     0                 0  \n",
       "19996               NaN  c111hz7     0                 0  \n",
       "19997               NaN  c111hz8     0                 0  \n",
       "19998           Atheist  c111hz9     0                 0  \n",
       "19999               NaN  c111hza     0                 0  \n",
       "\n",
       "[20000 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   parent_id               20000 non-null  object\n",
      " 1   created_utc             20000 non-null  int64 \n",
      " 2   author                  20000 non-null  object\n",
      " 3   subreddit_id            20000 non-null  object\n",
      " 4   score_hidden            20000 non-null  bool  \n",
      " 5   distinguished           1 non-null      object\n",
      " 6   subreddit               20000 non-null  object\n",
      " 7   score                   20000 non-null  int64 \n",
      " 8   body                    20000 non-null  object\n",
      " 9   archived                20000 non-null  bool  \n",
      " 10  name                    20000 non-null  object\n",
      " 11  link_id                 20000 non-null  object\n",
      " 12  gilded                  20000 non-null  int64 \n",
      " 13  edited                  20000 non-null  int64 \n",
      " 14  ups                     20000 non-null  int64 \n",
      " 15  retrieved_on            20000 non-null  int64 \n",
      " 16  author_flair_css_class  886 non-null    object\n",
      " 17  author_flair_text       668 non-null    object\n",
      " 18  id                      20000 non-null  object\n",
      " 19  downs                   20000 non-null  int64 \n",
      " 20  controversiality        20000 non-null  int64 \n",
      "dtypes: bool(2), int64(8), object(11)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>gilded</th>\n",
       "      <th>edited</th>\n",
       "      <th>ups</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>downs</th>\n",
       "      <th>controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.285896e+09</td>\n",
       "      <td>3.145350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.276667e+05</td>\n",
       "      <td>3.145350</td>\n",
       "      <td>1.426495e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.584719e+03</td>\n",
       "      <td>14.349642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.378336e+07</td>\n",
       "      <td>14.349642</td>\n",
       "      <td>5.800159e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.285891e+09</td>\n",
       "      <td>-35.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-35.000000</td>\n",
       "      <td>1.426495e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.285893e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.426495e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.285896e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.426495e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.285898e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.426495e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.285900e+09</td>\n",
       "      <td>845.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.423745e+09</td>\n",
       "      <td>845.000000</td>\n",
       "      <td>1.426495e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc         score   gilded        edited           ups  \\\n",
       "count  2.000000e+04  20000.000000  20000.0  2.000000e+04  20000.000000   \n",
       "mean   1.285896e+09      3.145350      0.0  8.276667e+05      3.145350   \n",
       "std    2.584719e+03     14.349642      0.0  3.378336e+07     14.349642   \n",
       "min    1.285891e+09    -35.000000      0.0  0.000000e+00    -35.000000   \n",
       "25%    1.285893e+09      1.000000      0.0  0.000000e+00      1.000000   \n",
       "50%    1.285896e+09      1.000000      0.0  0.000000e+00      1.000000   \n",
       "75%    1.285898e+09      2.000000      0.0  0.000000e+00      2.000000   \n",
       "max    1.285900e+09    845.000000      0.0  1.423745e+09    845.000000   \n",
       "\n",
       "       retrieved_on    downs  controversiality  \n",
       "count  2.000000e+04  20000.0           20000.0  \n",
       "mean   1.426495e+09      0.0               0.0  \n",
       "std    5.800159e+01      0.0               0.0  \n",
       "min    1.426495e+09      0.0               0.0  \n",
       "25%    1.426495e+09      0.0               0.0  \n",
       "50%    1.426495e+09      0.0               0.0  \n",
       "75%    1.426495e+09      0.0               0.0  \n",
       "max    1.426495e+09      0.0               0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parent_id                     0\n",
       "created_utc                   0\n",
       "author                        0\n",
       "subreddit_id                  0\n",
       "score_hidden                  0\n",
       "distinguished             19999\n",
       "subreddit                     0\n",
       "score                         0\n",
       "body                          0\n",
       "archived                      0\n",
       "name                          0\n",
       "link_id                       0\n",
       "gilded                        0\n",
       "edited                        0\n",
       "ups                           0\n",
       "retrieved_on                  0\n",
       "author_flair_css_class    19114\n",
       "author_flair_text         19332\n",
       "id                            0\n",
       "downs                         0\n",
       "controversiality              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('distinguished',axis=1,inplace=True)\n",
    "data.drop('author_flair_css_class',axis=1,inplace=True)\n",
    "data.drop('author_flair_text',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('score',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>archived</th>\n",
       "      <th>gilded</th>\n",
       "      <th>edited</th>\n",
       "      <th>ups</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>downs</th>\n",
       "      <th>controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>created_utc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.015254</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_hidden</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>archived</th>\n",
       "      <td>-0.003427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>-0.003486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gilded</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edited</th>\n",
       "      <td>0.000195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ups</th>\n",
       "      <td>0.015254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retrieved_on</th>\n",
       "      <td>0.999959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>downs</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>controversiality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_utc  score_hidden  archived  gilded    edited  \\\n",
       "created_utc          1.000000           NaN -0.003427     NaN  0.000195   \n",
       "score_hidden              NaN           NaN       NaN     NaN       NaN   \n",
       "archived            -0.003427           NaN  1.000000     NaN  0.001355   \n",
       "gilded                    NaN           NaN       NaN     NaN       NaN   \n",
       "edited               0.000195           NaN  0.001355     NaN  1.000000   \n",
       "ups                  0.015254           NaN  0.002077     NaN  0.006979   \n",
       "retrieved_on         0.999959           NaN -0.003486     NaN  0.000238   \n",
       "downs                     NaN           NaN       NaN     NaN       NaN   \n",
       "controversiality          NaN           NaN       NaN     NaN       NaN   \n",
       "\n",
       "                       ups  retrieved_on  downs  controversiality  \n",
       "created_utc       0.015254      0.999959    NaN               NaN  \n",
       "score_hidden           NaN           NaN    NaN               NaN  \n",
       "archived          0.002077     -0.003486    NaN               NaN  \n",
       "gilded                 NaN           NaN    NaN               NaN  \n",
       "edited            0.006979      0.000238    NaN               NaN  \n",
       "ups               1.000000      0.015260    NaN               NaN  \n",
       "retrieved_on      0.015260      1.000000    NaN               NaN  \n",
       "downs                  NaN           NaN    NaN               NaN  \n",
       "controversiality       NaN           NaN    NaN               NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.score_hidden.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.downs.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.controversiality.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('score_hidden',axis=1,inplace=True)\n",
    "data.drop('downs',axis=1,inplace=True)\n",
    "data.drop('controversiality',axis=1,inplace=True)\n",
    "data.drop('gilded',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.subreddit.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Can you give a reference for this. I don't bel...\n",
       "1                           I have this and it works well.\n",
       "2        I like Ron Paul, but I wish the guy would wear...\n",
       "3        To produce what we currently produce using the...\n",
       "4        This was shockingly coincidental, but I squirt...\n",
       "                               ...                        \n",
       "19995                   I didn't even know we had a cabin.\n",
       "19996    Show the ladies and acquire all the pussy with...\n",
       "19997       im looking to try this pretty soon. how is it?\n",
       "19998    &gt;*\"The Holy Ghost was so fucking cool. It m...\n",
       "19999                                       No, seriously.\n",
       "Name: body, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stemming=False, stop_words=True):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    stops = stopwords.words('english')\n",
    "    \n",
    "    # Empty comment\n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "    \n",
    "    # Commence the cleaning!\n",
    "    urls = r'http(s)*:\\/\\/(\\w|\\.)+(\\/\\w+)*'\n",
    "    text = re.sub(urls, '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are\", text)\n",
    "    text = re.sub(\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(\"\\'d\", \" would\", text)\n",
    "    text = re.sub(\"cant\", \"can not\", text)\n",
    "    text = re.sub(\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(\"isn\\'t\", \"is not\", text)\n",
    "    text = re.sub(\"isnt\", \"is not\", text)\n",
    "    text = re.sub(\"whats\", \"what is\", text)\n",
    "    text = re.sub(\"what\\'s\", \"what is\", text)\n",
    "    text = re.sub(\"shouldn't\", \"should not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"I'm\", \"I am\", text)\n",
    "    text = re.sub(\":\", \" \", text)\n",
    "    # The comments contain \\n for line breaks, we need to remove those too\n",
    "    text = re.sub(\"\\\\n\", \" \", text)\n",
    "    \n",
    "    # Special characters\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([word for word in text if word not in punctuation]).lower()\n",
    "    \n",
    "    # If we want to do stemming...\n",
    "    if stemming:\n",
    "        sno = SnowballStemmer('english')\n",
    "        text = ''.join([sno.stem[word] for word in text])\n",
    "    \n",
    "    # If we want to remove stop words...\n",
    "    if stop_words:\n",
    "        text = text.split()\n",
    "        text = [word for word in text if word not in stops]\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body'] = data['body'].apply(clean).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        give reference dont believe true ie constituti...\n",
       "1                                               works well\n",
       "2        like ron paul wish guy would wear regular shoe...\n",
       "3        produce currently produce using methods advoca...\n",
       "4        shockingly coincidental squirted first time 2 ...\n",
       "                               ...                        \n",
       "19995                                didnt even know cabin\n",
       "19996    show ladies acquire pussy within tribe magical...\n",
       "19997                           im looking try pretty soon\n",
       "19998    gtthe holy ghost fucking cool made adults look...\n",
       "19999                                            seriously\n",
       "Name: body, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data['body'] \n",
    "y=data.ups.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y, random_state=42, test_size=0.33, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13400,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6600,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tdidf=TfidfVectorizer(ngram_range=(1,1),stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_tdidf=tdidf.fit_transform(X_train)\n",
    "#X_test_tdidf=tdidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=2, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomforest =RandomForestClassifier(n_estimators=100,random_state=2)\n",
    "#randomforest.fit(X_train_tdidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred1 = randomforest.predict(X_test_tdidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import metrics\n",
    "#score = metrics.accuracy_score(Y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4546969696969697"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tdidf=TfidfVectorizer(ngram_range=(1,1),stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(r'{}.pkl'.format('up_vect'),'wb') as f:\n",
    " #   pickle.dump(tdidf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(r'{}.pkl'.format('up_model'),'wb')as f:\n",
    " #   pickle.dump(randomforest,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 40)           1077240   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               56400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,133,741\n",
      "Trainable params: 1,133,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_vector_features=40\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_vector_features,input_length= maxlen))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "210/210 [==============================] - 39s 186ms/step - loss: -36.3854 - accuracy: 0.4851 - val_loss: -70.7203 - val_accuracy: 0.4861\n",
      "Epoch 2/5\n",
      "210/210 [==============================] - 46s 220ms/step - loss: -76.1205 - accuracy: 0.4851 - val_loss: -114.6114 - val_accuracy: 0.4861\n",
      "Epoch 3/5\n",
      "210/210 [==============================] - 39s 186ms/step - loss: -112.5118 - accuracy: 0.4851 - val_loss: -158.7708 - val_accuracy: 0.4861\n",
      "Epoch 4/5\n",
      "210/210 [==============================] - 38s 179ms/step - loss: -148.6968 - accuracy: 0.4851 - val_loss: -202.1026 - val_accuracy: 0.4861\n",
      "Epoch 5/5\n",
      "210/210 [==============================] - 40s 190ms/step - loss: -183.8577 - accuracy: 0.4851 - val_loss: -244.8077 - val_accuracy: 0.4861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1273ea08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 6s 27ms/step - loss: -244.8077 - accuracy: 0.4861\n",
      "Test Score: -244.80770874023438\n",
      "Test Accuracy: 0.48606061935424805\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model.save('save_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
